<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Employing Bayesian Additive Regression Trees for Unemployment Rate Prediction | Blog</title>
<meta name="keywords" content="">
<meta name="description" content="Code in this example can be found on my github
Introduction Gradient boosting methods are commonly used in the Machine Learning field. It&rsquo;s a rather straight forward process as it utilized &ldquo;tree boosting&rdquo; optimization methods by combining random forest algorithms with a learning rate. Gradient boosting algorithms are seeking to minimize an objective function.
O ij = &#x2211; i = 1 I loss ( y i , y &#x007E; i ) &#x23DF; error term &#43; &#x2211; j = 1 J &#x03BB; ( T j ) &#x23DF; regularization term Most common machine learning algorithms are using a similar basic objective function which is based on a frequentist approach towards statistics.">
<meta name="author" content="Kai Brüning">
<link rel="canonical" href="https://contracourse.github.io/blogpage/docs/bart/">
<link crossorigin="anonymous" href="/blogpage/assets/css/stylesheet.3613efbd0b1772781e8f49935e973cae632a7f61471c05b17be155505ccf87b5.css" integrity="sha256-NhPvvQsXcngej0mTXpc8rmMqf2FHHAWxe&#43;FVUFzPh7U=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/blogpage/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://contracourse.github.io/blogpage/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://contracourse.github.io/blogpage/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://contracourse.github.io/blogpage/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://contracourse.github.io/blogpage/apple-touch-icon.png">
<link rel="mask-icon" href="https://contracourse.github.io/blogpage/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.css" integrity="sha384-3UiQGuEI4TTMaFmGIZumfRPtfKQ3trwQE2JgosJxCnGmQpL/lJdjpcHkaaFwHlcI" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.js" integrity="sha384-G0zcxDFp5LWZtDuRMnBkk3EphCK1lhEf4UEyEM693ka574TZGwo4IWwS6QLzM/2t" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>
<meta property="og:title" content="Employing Bayesian Additive Regression Trees for Unemployment Rate Prediction" />
<meta property="og:description" content="Code in this example can be found on my github
Introduction Gradient boosting methods are commonly used in the Machine Learning field. It&rsquo;s a rather straight forward process as it utilized &ldquo;tree boosting&rdquo; optimization methods by combining random forest algorithms with a learning rate. Gradient boosting algorithms are seeking to minimize an objective function.
O ij = &#x2211; i = 1 I loss ( y i , y &#x007E; i ) &#x23DF; error term &#43; &#x2211; j = 1 J &#x03BB; ( T j ) &#x23DF; regularization term Most common machine learning algorithms are using a similar basic objective function which is based on a frequentist approach towards statistics." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://contracourse.github.io/blogpage/docs/bart/" /><meta property="article:section" content="docs" />
<meta property="article:published_time" content="2023-04-09T18:29:50+02:00" />
<meta property="article:modified_time" content="2023-04-09T18:29:50+02:00" /><meta property="og:site_name" content="Kais Blog" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Employing Bayesian Additive Regression Trees for Unemployment Rate Prediction"/>
<meta name="twitter:description" content="Code in this example can be found on my github
Introduction Gradient boosting methods are commonly used in the Machine Learning field. It&rsquo;s a rather straight forward process as it utilized &ldquo;tree boosting&rdquo; optimization methods by combining random forest algorithms with a learning rate. Gradient boosting algorithms are seeking to minimize an objective function.
O ij = &#x2211; i = 1 I loss ( y i , y &#x007E; i ) &#x23DF; error term &#43; &#x2211; j = 1 J &#x03BB; ( T j ) &#x23DF; regularization term Most common machine learning algorithms are using a similar basic objective function which is based on a frequentist approach towards statistics."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Docs",
      "item": "https://contracourse.github.io/blogpage/docs/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Employing Bayesian Additive Regression Trees for Unemployment Rate Prediction",
      "item": "https://contracourse.github.io/blogpage/docs/bart/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Employing Bayesian Additive Regression Trees for Unemployment Rate Prediction",
  "name": "Employing Bayesian Additive Regression Trees for Unemployment Rate Prediction",
  "description": "Code in this example can be found on my github\nIntroduction Gradient boosting methods are commonly used in the Machine Learning field. It\u0026rsquo;s a rather straight forward process as it utilized \u0026ldquo;tree boosting\u0026rdquo; optimization methods by combining random forest algorithms with a learning rate. Gradient boosting algorithms are seeking to minimize an objective function.\nO ij = \u0026#x2211; i = 1 I loss ( y i , y \u0026#x007E; i ) \u0026#x23DF; error term + \u0026#x2211; j = 1 J \u0026#x03BB; ( T j ) \u0026#x23DF; regularization term Most common machine learning algorithms are using a similar basic objective function which is based on a frequentist approach towards statistics.",
  "keywords": [
    
  ],
  "articleBody": " Code in this example can be found on my github\nIntroduction Gradient boosting methods are commonly used in the Machine Learning field. It’s a rather straight forward process as it utilized “tree boosting” optimization methods by combining random forest algorithms with a learning rate. Gradient boosting algorithms are seeking to minimize an objective function.\nO ij = ∑ i = 1 I loss ( y i , y ~ i ) ⏟ error term + ∑ j = 1 J λ ( T j ) ⏟ regularization term Most common machine learning algorithms are using a similar basic objective function which is based on a frequentist approach towards statistics. The Bayesian approach treats the models in terms of a probability distribution, instead of giving you an exact output parameter.\nWe can use a Bayesian approach to determine the model parameters. This approach allows us to incorporate our prior beliefs about the shape of trees and the overall ensemble structure into the model. After that, the algorithm will update the priors based on the data using an MCMC back-fitting technique. Monte Carlo Markov chain (MCMC) is commonly used in Bayesian statistics to approximate complex distributions in order to estimate the posterior distribution of model parameters. For further details see here\nBayesian Additive Regression Trees (BART) utilizes similar tree boosting methods but through a Bayesian framework where predictions are drawn from a posterior distribution, which is a probability distribution of model parameters given the observed data. The BART model can be expressed as follows:\n$$ \\begin{equation} Y = f(X) + \\varepsilon ≈ T^M_1 (X) + T^M_2 (X) + . . . + T^M_m (X) + \\varepsilon, \\hspace{0.5cm} \\varepsilon ∼ N_n (0, σ^2I_n) \\end{equation} $$\n\\(Y\\) represent a \\(n * 1 \\) vector of the response variable and \\(X\\) is the \\(n * p \\) matrix of predictor columns. The \\(\\epsilon\\) is the error vector. The \\(m\\) is the number of regression trees composed from the tree structure denoted as \\(T\\), and the terminal nodes (leaves) denoted by \\(M\\), representing together an entire tree as \\(T^M\\) with the structure and leaf parameter.\nAdditive regression trees have splitting nodes that gives you smaller prediction spaces by adding them up leaving a better picture of the overall prediction space. You have a bunch of weak learners to get a clear picture of the whole. You are taking each small part of the regression tree that outputs a weak learner by itself and adding them up to get a bigger picture. To address the overfitting problem, the Bayesian approach penalizes against it through regularization priors. In the same way the regularization term does for the boosting algorithms. Data goes through each tree and only the residual flows to the next tree, the regularization prior balances the data in order to prevent overfitting.\nBART uses non-parametric regression models which is commonly used when relationships between variables are more complex and difficult to express. Non-parametric regression can also be useful for exploratory data analysis, as they can help to identify patterns and relationships that may not be apparent with simple summary statistics.\nApproach I’ve gathered some economic data from the FRED site (the “fredr” R-package allows you to access the Fred database via an API) and I’m trying to develop a BART algorithm to forecast the unemployment rate for America.\nMy dataset can be seen below. Observation starting from 2004 until the end of 2022; the UNRATE is the predictive variable \\(\\hat{y}\\) with the remaining independent variables \\(x\\).\n\u003e str(data_frame) tibble [228 × 8] (S3: tbl_df/tbl/data.frame) $ date : Date[1:228], format: \"2004-01-01\" \"2004-02-01\" ... $ UNRATE (unemployment rate) : num [1:228] 5.7 5.6 5.8 5.6 5.6 5.6 5.5 5.4 5.4 5.5 ... $ T10Y3M (10Y-3M spread) : num [1:228] 3.25 3.14 2.87 3.39 3.68 3.45 3.14 2.78 2.44 2.3 ... $ STLFSI4 (Fed St. Louis Stress Index) : num [1:228] -0.42 -0.494 -0.446 -0.597 -0.501 ... $ EFFR (Effective Federal Funds Rate) : num [1:228] 1 1.01 1 1.01 1 1.03 1.27 1.43 1.62 1.76 ... $ T5YIFR (5y5y Forward Infl. Expectations) : num [1:228] 2.49 2.43 2.45 2.52 2.74 2.65 2.55 2.49 2.4 2.36 ... $ REAINTRATREARAT1YE (1-year Real Interest Rate) : num [1:228] -0.1377 -0.4014 -0.0643 -0.4123 -0.5069 ... $ CPIAUCSL (CPI-Index) : num [1:228] 2.03 1.69 1.74 2.29 2.9 ... I split the data into training and testing using the R package “caret”, 20% of the data will go into the test set, while the remaining 80% will go into the training set.\nlibrary(caret) y \u003c- data$UNRATE df \u003c- within(data, rm(UNRATE)) set.seed(42) test_inds = createDataPartition(y = 1:length(y), p = 0.2, list = F) Now running bartMachine on the training data bart_machine = bartMachine(df_train, y_train). The default MCMC uses a burn-in rate of 250 and 1000 iteration with 50 trees. All of those parameters can be specified manually. BART uses L1 \u0026 L2 regularization to reduce overfitting, introduce penalties and reduce complexity especially with high dimensional data. The Pseudo-Rsq is for non-linear models, it has the same interpretability as a normal R-squared.\nThe p-value of the shapiro-wilk test tells us about the data distribution. If the p-value is less than or equal to the significance level (usually 0.05), then we reject the null hypothesis and conclude that the data is not normally distributed.\n\u003e summary(bart_machine) bartMachine v1.3.3.1 for regression training data size: n = 180 and p = 6 built in 1.1 secs on 3 cores, 50 trees, 250 burn-in and 1000 post. samples sigsq est for y beforehand: 2.141 avg sigsq estimate after burn-in: 0.06986 in-sample statistics: L1 = 17.18 L2 = 2.65 rmse = 0.12 Pseudo-Rsq = 0.9967 p-val for shapiro-wilk test of normality of residuals: 0.81473 p-val for zero-mean noise: 0.91896 We can use the “rmse_by_num_trees” function to find the optimum number of trees for the model. I’ve given it a sequence from 15 to 75 trees by 5 increments with 3 number of replicant trees. The RMSE tree chart is used in order to illustrate the out-of-sample predictive capacity of our model. With additional hyperparameter optimization we can build a better bartmachine model in the future. As you can see it shows us the path of the trees with its respective RMSE. Then we can use the trees with the minimum RMSE and run the bartmachine again. The tree looks pretty static, an increase in the number of trees did not particularly perform better. bart_machine \u003c- bartMachine(df_train, y_train, num_trees=20)\nUsing the “plot_convergence_diagnostics” function we can see how the MCMC performs. Overall, the tree nodes perform relatively constant. On the top left, the Siqsq estimate converges after ~200 interactions inside the interval. The three subsequent plots separated by gray lines are the post-burn-in iterations from each of the three computing cores employed during the model.\nNext up, the “check_bart_error_assumptions” chart show us the error normality distribution using QQ-plots. We can see the residuals are normally distributed, no need of any adjustment.\nLastly, we will see how well our model performs in-sample and out-of-sample.\nBayesian statistics uses Credible intervals instead of Confidence intervals. Credible intervals provide a range of values where we can be certain that the true parameter lies within, given the available data and the assumptions of the model. The width of a credible interval reflects the amount of uncertainty in the prediction, with wider intervals indicating higher uncertainty. We can manually adjust the interval (default is 95%), a low interval means a wider grey bar of the uncertainty in the predictor. The out of sample chart is more important since it gives us an impression how well the model performs on the testing data. For the out-of-sample chart we can now provide a predictive range of each data point which is about 85% accurate given a 90% credible interval.\nThe Prediction intervals are drawn from the posterior distribution of the MCMC process described earlier, they are wider than the Credible intervals since they reflecting the uncertainty of the error term. Prediction intervals tells us about the precision of our individual predictions, a Credible interval gives us information about the likely range of true parameter values. As you can see below, the posterior of each prediction is getting larger as the unemployment rate increases. This reflects the uncertainty around the prediction value on the independent variable. If the distribution is wide or has high variance, then the posterior for that prediction will be larger. This is common in models with high levels of noise or when the data points are widely spread out. Therefore, it is important to consider the overall distribution of the target values and the model’s accuracy when interpreting the size of the posterior. I’ve chosen a CI of 90% since the posterior distributions are getting larger with a higher unemployment rates, reflecting the high degree of uncertainty around these values. A lower CI results in less predictive values that are being captured by the model and resulting in a lower boundary or limit of the prediction range.\nLastly, we can calculate some metrics to compare our bartmachine precitions with the testset. I’ve inserted the out-of-sample prediction into a dataframe and then calculated the ratios below. y_pred \u003c- predict(bart_machine, df_test)\nsummary(lm(y_test~y_pred))$r.squared sqrt(mean((y_test - y_pred)^2)) cor.test(y_test, y_pred, method=c(\"pearson\")) # Pearson R-squared [1] 0.9364641 # R-squared [1] 0.5188173 # RMSE Pearson's product-moment correlation data: y_test and y_pred t = 26.038, df = 46, p-value \u003c 2.2e-16 alternative hypothesis: true correlation is not equal to 0 95 percent confidence interval: 0.9428118 0.9818702 sample estimates: cor 0.9677108 Additional Diagnostic Charts In addition, bartmachine allows us to see the same kind of variable importance plot as xgboost. Running the “investigate_var_importance” function to see which variable has the most significant influence on the model’s predictions. Then we can choose the most important variable (in this case the EFFR-rate) and look at the partial dependence plot. The PD-Plot shows us how the target variable’s predicted values change as the selected input variable varies. The PD is plotted in black and a default 95% credible intervals plotted in blue for the other variables in the dataset. Points plotted are at the 5%ile, 10%ile, 20%ile, . . . , 60%ile and 75%ile of the values of the predictor. We can see as the EFFR increases the partial effect decreases. Which would be a negative relationship between the input variable and the predictor variable. In other words, as the value of X increases, the predicted value decreases. This indicates that the increased Federal Funds rate is positively associated with higher unemployment. Note: It is important to note that interpreting partial dependence plots requires considering the specific context of the data and the model.\nConclusion Overall, BART is an interesting algorithm with some unique capabilities, but their suitability will depend on the complexity of the dataset and the task at hand. Sometimes a Bayesian approach is preferred since it does not find the single best value, rather a range of possible values determined by the posterior distribution. BART can be compared to gradient algorithms like XGBoost or Catboost. BART may be more appropriate when dealing with complex nonlinear relationships, while XGBoost may be better suited for simpler problems where speed and scalability are important.\nReferences Coqueret, G., \u0026 Guida, T. (2022, October 18). Machine Learning for Factor Investing. http://www.mlfactor.com/bayes.html\nKapelner, A., \u0026 Bleich, J. (2016). bartMachine: Machine Learning with Bayesian Additive Regression Trees. Journal of Statistical Software, 70(4). https://doi.org/10.18637/jss.v070.i04\nKoehrsen, W. (2018, April 14). Introduction to Bayesian Linear Regression. https://towardsdatascience.com/introduction-to-bayesian-linear-regression-e66e60791ea7\nMamun, O. (2021, May 3). A Primer to Bayesian Additive Regression Tree with R. https://towardsdatascience.com/a-primer-to-bayesian-additive-regression-tree-with-r-b9d0dbf704d\n",
  "wordCount" : "1915",
  "inLanguage": "en",
  "datePublished": "2023-04-09T18:29:50+02:00",
  "dateModified": "2023-04-09T18:29:50+02:00",
  "author":{
    "@type": "Person",
    "name": "Kai Brüning"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://contracourse.github.io/blogpage/docs/bart/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://contracourse.github.io/blogpage/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://contracourse.github.io/blogpage/" accesskey="h" title="Blog (Alt + H)">Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://contracourse.github.io/blogpage/docs/" title="Blogs">
                    <span>Blogs</span>
                </a>
            </li>
            <li>
                <a href="https://contracourse.github.io/blogpage/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="https://contracourse.github.io/blogpage/about/" title="About">
                    <span>About</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://contracourse.github.io/blogpage/">Home</a>&nbsp;»&nbsp;<a href="https://contracourse.github.io/blogpage/docs/">Docs</a></div>
    <h1 class="post-title">
      Employing Bayesian Additive Regression Trees for Unemployment Rate Prediction
    </h1>
    <div class="post-meta"><span title='2023-04-09 18:29:50 +0200 +0200'>April 9, 2023</span>&nbsp;·&nbsp;9 min&nbsp;·&nbsp;Kai Brüning

</div>
  </header> 
  <div class="post-content"><span style="font-size:16px;">
<p><em>Code in this example can be found on my <a href="https://github.com/contracourse/Bayesian_Regression">github</a></em></p>
<h2 id="introduction">Introduction<a hidden class="anchor" aria-hidden="true" href="#introduction">#</a></h2>
<p>Gradient boosting methods are commonly used in the Machine Learning
field. It&rsquo;s a rather straight forward process as it utilized &ldquo;tree
boosting&rdquo; optimization methods by combining random forest algorithms
with a learning rate. Gradient boosting algorithms are seeking to
minimize an objective function.</p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mi>O</mi>
          <msub>
            <mi></mi>
            <mi>ij</mi>
          </msub>
  <mo>=</mo>
  <munder>
    <mrow class="MJX-TeXAtom-OP MJX-fixedlimits">
      <munder>
        <mrow>
          <munderover>
            <mo>&#x2211;<!-- ∑ --></mo>
            <mrow class="MJX-TeXAtom-ORD">
              <mi>i</mi>
              <mo>=</mo>
              <mn>1</mn>
            </mrow>
            <mi>I</mi>
          </munderover>
          <mtext>loss</mtext>
          <mo stretchy="false">(</mo>
          <msub>
            <mi>y</mi>
            <mi>i</mi>
          </msub>
          <mo>,</mo>
          <msub>
            <mrow class="MJX-TeXAtom-ORD">
              <mover>
                <mi>y</mi>
                <mo stretchy="false">&#x007E;<!-- ~ --></mo>
              </mover>
            </mrow>
            <mi>i</mi>
          </msub>
          <mo stretchy="false">)</mo>
        </mrow>
        <mo>&#x23DF;<!-- ⏟ --></mo>
      </munder>
    </mrow>
    <mrow class="MJX-TeXAtom-ORD">
      <mtext>error term</mtext>
    </mrow>
  </munder>
  <mspace width="1em" />
  <mo>+</mo>
  <munder>
    <mrow class="MJX-TeXAtom-OP MJX-fixedlimits">
      <munder>
        <mrow>
          <munderover>
            <mo>&#x2211;<!-- ∑ --></mo>
            <mrow class="MJX-TeXAtom-ORD">
              <mi>j</mi>
              <mo>=</mo>
              <mn>1</mn>
            </mrow>
            <mi>J</mi>
          </munderover>
          <mi mathvariant="normal">&#x03BB;<!-- λ --></mi>
          <mo stretchy="false">(</mo>
          <msub>
            <mi>T</mi>
            <mi>j</mi>
          </msub>
          <mo stretchy="false">)</mo>
        </mrow>
        <mo>&#x23DF;<!-- ⏟ --></mo>
      </munder>
    </mrow>
    <mrow class="MJX-TeXAtom-ORD">
      <mtext>regularization term</mtext>
    </mrow>
  </munder>
</math>
<p>Most common machine learning algorithms are using a similar basic
objective function which is based on a <em><strong>frequentist approach</strong></em>
towards statistics. The Bayesian approach treats the models in terms of
a probability distribution, instead of giving you an exact output
parameter.</p>
<p>We can use a Bayesian approach to determine the model parameters. This
approach allows us to incorporate our prior beliefs about the shape of
trees and the overall ensemble structure into the model. After that, the
algorithm will update the priors based on the data using an MCMC
back-fitting technique. Monte Carlo Markov chain (MCMC) is commonly used in
Bayesian statistics to approximate complex distributions in order to estimate
the posterior distribution of model parameters. For further details see
<a href="https://towardsdatascience.com/a-zero-math-introduction-to-markov-chain-monte-carlo-methods-dcba889e0c50">here</a></p>
<p>Bayesian Additive Regression Trees <em><u>(BART)</u></em> utilizes similar tree
boosting methods but through a Bayesian framework where predictions are drawn
from a posterior distribution, which is a probability distribution of model
parameters given the observed data. The BART model can be expressed as follows:</p>
<p>$$
\begin{equation}
Y = f(X) + \varepsilon ≈ T^M_1
(X) + T^M_2
(X) + . . . + T^M_m (X) + \varepsilon,
\hspace{0.5cm}
\varepsilon ∼ N_n
(0, σ^2I_n)
\end{equation}
$$</p>
<p>\(Y\) represent a \(n * 1 \)  vector of the response variable and \(X\) is the \(n * p \) matrix of
predictor columns. The \(\epsilon\) is the error vector. The \(m\) is the number of regression
trees composed from the tree structure denoted as \(T\), and the terminal nodes
(leaves) denoted by \(M\), representing together an entire tree as \(T^M\) with the
structure and leaf parameter.</p>
<p>Additive regression trees have splitting nodes that gives you smaller
prediction spaces by adding them up leaving a better
picture of the overall prediction space. You have a bunch of weak
learners to get a clear picture of the whole. You are taking each small
part of the regression tree that outputs a weak learner by itself and
adding them up to get a bigger picture. To address the overfitting
problem, the Bayesian approach penalizes against it through
regularization priors. In the same way the regularization term does for the
boosting algorithms. Data goes through each tree and only the residual flows
to the next tree, the regularization prior balances the data in order to
prevent overfitting.</p>
<p>BART uses non-parametric regression models which is commonly used when
relationships between variables are more complex and difficult to
express. Non-parametric regression can also be useful for exploratory data
analysis, as they can help to identify patterns and relationships that
may not be apparent with simple summary statistics.</p>
<h2 id="approach">Approach<a hidden class="anchor" aria-hidden="true" href="#approach">#</a></h2>
<p>I’ve gathered some economic data from the FRED site (the “fredr” R-package
allows you to access the Fred database via an API) and I’m trying to develop a
BART algorithm to forecast the unemployment rate for America.</p>
<p>My dataset can be
seen below. Observation starting from 2004 until the end of 2022; the UNRATE is the predictive
variable \(\hat{y}\) with the remaining independent variables \(x\).</p>
<pre tabindex="0"><code>&gt; str(data_frame)
tibble [228 × 8] (S3: tbl_df/tbl/data.frame)
 $ date                                                       : Date[1:228], format: &#34;2004-01-01&#34; &#34;2004-02-01&#34; ...
 $ UNRATE             (unemployment rate)                     : num [1:228] 5.7 5.6 5.8 5.6 5.6 5.6 5.5 5.4 5.4 5.5 ...
 $ T10Y3M             (10Y-3M spread)                         : num [1:228] 3.25 3.14 2.87 3.39 3.68 3.45 3.14 2.78 2.44 2.3 ...
 $ STLFSI4            (Fed St. Louis Stress Index)            : num [1:228] -0.42 -0.494 -0.446 -0.597 -0.501 ...
 $ EFFR               (Effective Federal Funds Rate)          : num [1:228] 1 1.01 1 1.01 1 1.03 1.27 1.43 1.62 1.76 ...
 $ T5YIFR             (5y5y Forward Infl. Expectations)       : num [1:228] 2.49 2.43 2.45 2.52 2.74 2.65 2.55 2.49 2.4 2.36 ...
 $ REAINTRATREARAT1YE (1-year Real Interest Rate)             : num [1:228] -0.1377 -0.4014 -0.0643 -0.4123 -0.5069 ...
 $ CPIAUCSL           (CPI-Index)                             : num [1:228] 2.03 1.69 1.74 2.29 2.9 ...
</code></pre><p>I split the data into training and testing using the R package “caret”,
20% of the data will go into the test set, while the remaining 80% will go into
the training set.</p>
<pre tabindex="0"><code>library(caret)
y &lt;- data$UNRATE
df &lt;- within(data, rm(UNRATE))
set.seed(42) 
test_inds = createDataPartition(y = 1:length(y), p = 0.2, list = F)
</code></pre><p>Now running bartMachine on the training data <code>bart_machine = bartMachine(df_train, y_train)</code>. The default MCMC uses a burn-in rate of
250 and 1000 iteration with 50 trees. All of those parameters can be specified
manually. <br>
BART uses L1 &amp; L2 regularization to reduce overfitting, introduce penalties and
reduce complexity especially with high dimensional data. The Pseudo-Rsq is for
non-linear models, it has the same interpretability as a normal R-squared.</p>
<p>The p-value of the shapiro-wilk test tells us about
the data distribution. If the p-value is less than or equal to the significance
level (usually 0.05), then we reject the null hypothesis and conclude that the
data is not normally distributed.</p>
<pre tabindex="0"><code>&gt; summary(bart_machine)
bartMachine v1.3.3.1 for regression

training data size: n = 180 and p = 6 
built in 1.1 secs on 3 cores, 50 trees, 250 burn-in and 1000 post. samples

sigsq est for y beforehand: 2.141 
avg sigsq estimate after burn-in: 0.06986 

in-sample statistics:
 L1 = 17.18 
 L2 = 2.65 
 rmse = 0.12 
 Pseudo-Rsq = 0.9967
p-val for shapiro-wilk test of normality of residuals: 0.81473 
p-val for zero-mean noise: 0.91896 
</code></pre><p>We can use the “rmse_by_num_trees” function to find the optimum number of trees
for the model. I’ve given it a sequence from 15 to 75 trees by 5 increments with
3 number of replicant trees. The RMSE tree chart is used in order to illustrate
the out-of-sample predictive capacity of our model. With additional
hyperparameter optimization we can build a better bartmachine model in the
future. <br></p>
<p><img loading="lazy" src="https://raw.githubusercontent.com/contracourse/blogpage/1bf4db9d5b37636a0c5e4e1001ce7d1fb206fc2d/static/images/rmse_by_num_trees.svg" alt="RMSE
Tree-Plot"  />
</p>
<p>As you can see it shows us the path of the trees with its respective RMSE. Then
we can use the trees with the minimum RMSE and run the <code>bartmachine</code> again. The
tree looks pretty static, an increase in the number of trees did not particularly
perform better. <br>
<code>bart_machine &lt;- bartMachine(df_train, y_train, num_trees=20)</code></p>
<p>Using the “plot_convergence_diagnostics” function we can see how the MCMC
performs. Overall, the tree nodes perform relatively constant. On the top left,
the Siqsq estimate converges after ~200 interactions inside the interval.
The three subsequent plots separated by gray lines are the post-burn-in
iterations from each of the three computing cores employed during the model.</p>
<p><img loading="lazy" src="https://raw.githubusercontent.com/contracourse/blogpage/d8166cedb681f34c95e01273ca188e3694ed9d93/static/images/plot_convergence_diagnostics.svg" alt="Plot-Diagnostics"  />
</p>
<p>Next up, the “check_bart_error_assumptions” chart show us the error normality
distribution using QQ-plots. We can see the residuals are normally distributed,
no need of any adjustment.</p>
<p><img loading="lazy" src="https://raw.githubusercontent.com/contracourse/blogpage/d8166cedb681f34c95e01273ca188e3694ed9d93/static/images/check_bart_error_assumptions.svg" alt="QQ-Plot"  />
</p>
<p>Lastly, we will see how well our model performs in-sample and out-of-sample.</p>
<p>Bayesian statistics uses Credible intervals instead of Confidence intervals.
Credible intervals provide a range of values where we can be certain that the
true parameter lies within, given the available data and the assumptions of the
model. The width of a credible interval reflects the amount of uncertainty in
the prediction, with wider intervals indicating higher uncertainty. We can
manually adjust the interval (default is 95%), a low interval means a wider
grey bar of the uncertainty in the predictor. The out of sample chart is more
important since it gives us an impression how well the model performs on the
testing data. For the out-of-sample chart we can now provide a predictive
range of each data point which is about 85% accurate given a 90% credible interval.</p>
<p>The Prediction intervals are drawn from the posterior distribution of the MCMC
process described earlier, they are wider than the Credible intervals since they
reflecting the uncertainty of the error term. Prediction intervals tells us
about the precision of our individual predictions, a Credible interval gives us
information about the likely range of true parameter values. As you can see
below, the posterior of each prediction is getting larger as the unemployment
rate increases. This reflects the uncertainty around the prediction value on the
independent variable.  If the distribution is wide or has high variance, then
the posterior for that prediction will be larger. This is common in models with
high levels of noise or when the data points are widely spread out. Therefore,
it is important to consider the overall distribution of the target values and
the model&rsquo;s accuracy when interpreting the size of the posterior. <br> I&rsquo;ve
chosen a CI of 90% since the posterior distributions are getting larger with a
higher unemployment rates, reflecting the high degree of uncertainty around
these values. A lower CI results in less predictive values that are being
captured by the model and resulting in a lower boundary or limit of the
prediction range.</p>
<p><img loading="lazy" src="https://raw.githubusercontent.com/contracourse/blogpage/985cdbc3d4c208bb1341c45f020214089e3eab1a/static/images/plot_y_vs_yhat_2.svg" alt="plot-y vs y-hat"  />
</p>
<p>Lastly, we can calculate some metrics to compare our bartmachine precitions with the testset.
I&rsquo;ve inserted the out-of-sample prediction into a dataframe and then calculated the ratios below. <br>
<code>y_pred &lt;- predict(bart_machine, df_test)</code></p>
<pre tabindex="0"><code>summary(lm(y_test~y_pred))$r.squared
sqrt(mean((y_test - y_pred)^2))
cor.test(y_test, y_pred, method=c(&#34;pearson&#34;))   # Pearson R-squared  

[1] 0.9364641 # R-squared 
[1] 0.5188173 # RMSE

        Pearson&#39;s product-moment correlation

data:  y_test and y_pred
t = 26.038, df = 46, p-value &lt; 2.2e-16
alternative hypothesis: true correlation is not equal to 0
95 percent confidence interval:
 0.9428118 0.9818702
sample estimates:
      cor 
0.9677108 
</code></pre><h3 id="additional-diagnostic-charts">Additional Diagnostic Charts<a hidden class="anchor" aria-hidden="true" href="#additional-diagnostic-charts">#</a></h3>
<p>In addition, bartmachine allows us to see the same kind of variable importance
plot as xgboost. Running the “investigate_var_importance” function to see which
variable has the most significant influence on the model&rsquo;s predictions.
<img loading="lazy" src="https://raw.githubusercontent.com/contracourse/blogpage/2ca75f5a98ad698ad2e4cef0b281ba7b3179cca7/static/images/importance%20plot.svg" alt="var_importance"  />
</p>
<p>Then we can choose the most important variable (in this case the EFFR-rate) and
look at the partial dependence plot. The PD-Plot shows us how the target variable&rsquo;s
predicted values change as the selected input variable varies. The
PD is plotted in black and a default 95% credible intervals plotted in blue for
the other  variables in the dataset. Points plotted are at the 5%ile, 10%ile,
20%ile, . . . , 60%ile and 75%ile of the values of the predictor. We can see as
the EFFR increases the partial effect decreases. Which would be a negative
relationship between the input variable and the predictor variable. In other
words, as the value of X increases, the predicted value decreases. This
indicates that the increased Federal Funds rate is positively associated with
higher unemployment. <br> <em>Note: It is important to note that interpreting partial
dependence plots requires considering the specific context of the data and the
model.</em></p>
<p><img loading="lazy" src="https://raw.githubusercontent.com/contracourse/blogpage/2ca75f5a98ad698ad2e4cef0b281ba7b3179cca7/static/images/pd_plot.svg" alt="pd-plot"  />
</p>
<h2 id="conclusion">Conclusion<a hidden class="anchor" aria-hidden="true" href="#conclusion">#</a></h2>
<p>Overall, BART is an interesting algorithm with some unique capabilities, but
their suitability will depend on the complexity of the dataset and the task at
hand. Sometimes a Bayesian approach is preferred since it does not find the
single best value, rather a range of possible values determined by the posterior
distribution. <br></p>
<p>BART can be compared to gradient algorithms like XGBoost or Catboost. BART may
be more appropriate when dealing with complex nonlinear relationships, while
XGBoost may be better suited for simpler problems where speed and scalability
are important.</p>
<h1 id='references'>References</h1>
<p>Coqueret, G., &amp; Guida, T. (2022, October 18). <em>Machine Learning for
Factor Investing</em>. <a href="http://www.mlfactor.com/bayes.html">http://www.mlfactor.com/bayes.html</a></p>
<p>Kapelner, A., &amp; Bleich, J. (2016). <strong>bartMachine</strong>: Machine Learning
with Bayesian Additive Regression Trees. <em>Journal of Statistical
Software</em>, <em>70</em>(4). <a href="https://doi.org/10.18637/jss.v070.i04">https://doi.org/10.18637/jss.v070.i04</a></p>
<p>Koehrsen, W. (2018, April 14). <em>Introduction to Bayesian Linear
Regression</em>.
<a href="https://towardsdatascience.com/introduction-to-bayesian-linear-regression-e66e60791ea7">https://towardsdatascience.com/introduction-to-bayesian-linear-regression-e66e60791ea7</a></p>
<p>Mamun, O. (2021, May 3). <em>A Primer to Bayesian Additive Regression Tree
with R</em>.
<a href="https://towardsdatascience.com/a-primer-to-bayesian-additive-regression-tree-with-r-b9d0dbf704d">https://towardsdatascience.com/a-primer-to-bayesian-additive-regression-tree-with-r-b9d0dbf704d</a></p>
</span>

  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
<nav class="paginav">
  <a class="prev" href="https://contracourse.github.io/blogpage/docs/banking/">
    <span class="title">« Prev</span>
    <br>
    <span>Banking</span>
  </a>
  <a class="next" href="https://contracourse.github.io/blogpage/docs/test/">
    <span class="title">Next »</span>
    <br>
    <span>Test</span>
  </a>
</nav>


<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share Employing Bayesian Additive Regression Trees for Unemployment Rate Prediction on twitter"
        href="https://twitter.com/intent/tweet/?text=Employing%20Bayesian%20Additive%20Regression%20Trees%20for%20Unemployment%20Rate%20Prediction&amp;url=https%3a%2f%2fcontracourse.github.io%2fblogpage%2fdocs%2fbart%2f&amp;hashtags=">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Employing Bayesian Additive Regression Trees for Unemployment Rate Prediction on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fcontracourse.github.io%2fblogpage%2fdocs%2fbart%2f&amp;title=Employing%20Bayesian%20Additive%20Regression%20Trees%20for%20Unemployment%20Rate%20Prediction&amp;summary=Employing%20Bayesian%20Additive%20Regression%20Trees%20for%20Unemployment%20Rate%20Prediction&amp;source=https%3a%2f%2fcontracourse.github.io%2fblogpage%2fdocs%2fbart%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Employing Bayesian Additive Regression Trees for Unemployment Rate Prediction on reddit"
        href="https://reddit.com/submit?url=https%3a%2f%2fcontracourse.github.io%2fblogpage%2fdocs%2fbart%2f&title=Employing%20Bayesian%20Additive%20Regression%20Trees%20for%20Unemployment%20Rate%20Prediction">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Employing Bayesian Additive Regression Trees for Unemployment Rate Prediction on facebook"
        href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fcontracourse.github.io%2fblogpage%2fdocs%2fbart%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Employing Bayesian Additive Regression Trees for Unemployment Rate Prediction on whatsapp"
        href="https://api.whatsapp.com/send?text=Employing%20Bayesian%20Additive%20Regression%20Trees%20for%20Unemployment%20Rate%20Prediction%20-%20https%3a%2f%2fcontracourse.github.io%2fblogpage%2fdocs%2fbart%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Employing Bayesian Additive Regression Trees for Unemployment Rate Prediction on telegram"
        href="https://telegram.me/share/url?text=Employing%20Bayesian%20Additive%20Regression%20Trees%20for%20Unemployment%20Rate%20Prediction&amp;url=https%3a%2f%2fcontracourse.github.io%2fblogpage%2fdocs%2fbart%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023 <a href="https://contracourse.github.io/blogpage/">Blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
